{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "433f2312",
   "metadata": {},
   "source": [
    "# Regularization method list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6d570d",
   "metadata": {},
   "source": [
    "## I. Normalization\n",
    "### **Batchnorm**\n",
    "Normalizes the activations of each layer in a mini-batch.\n",
    "### **Layernorm**\n",
    "Normalizes the activations across the feature dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d516e844",
   "metadata": {},
   "source": [
    "## II. Weight\n",
    "### II.1 Weight Decay(L1 and L2 Regularization)\n",
    "1. regularization的作用 \\\n",
    "(1) 引入对w取值的偏好，本质上是把human prior knowledge注入模型的方式。比如L2体现diffuse weights，L1 norm体现feature select的想法。\\\n",
    "(2) 减少overfit。给模型提供额外的目标，避免只去fit data，也因此能让outlier的影响变小。\\\n",
    "(3) improve optimization by adding curvature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e07b7d",
   "metadata": {},
   "source": [
    "### II.2 **Max-Norm: $∥w∥_2≤c$**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33055eba",
   "metadata": {},
   "source": [
    "### II.3 **Weight clipping**\n",
    "1. **weight clipping和max-norm的区别**\\\n",
    "(1)**Granularity**: Max-norm regularization constrains the entire weight vector's magnitude. Weight clipping applies a per-parameter constraint.\\\n",
    "(2)**Smoothness**: Max-norm regularization provides a smooth and continuous constraint on the weights. Weight clipping can result in a discontinuity in the weight space.\\\n",
    "(3)**Stability**: Max-norm regularization maintains the direction of the weight vector while potentially adjusting its magnitude. Weight clipping can change both the direction and magnitude of the weight vector.\n",
    "2. **Weight Tying**: Constrains certain weights to be equal, reducing the number of parameters.\n",
    "3. **Weight Reg on Activations (WAR)**: Penalizes the squared magnitude of activations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b98ac9a",
   "metadata": {},
   "source": [
    "## III. **Dropout**:\n",
    "### III.1 vanilla dropout: \n",
    "Randomly sets a fraction of the neurons to zero during training\n",
    "### III.2 DropConnect:\n",
    "Randomly sets a fraction of weights to zero within a layer.\n",
    "### III.3 Stochastic Depth: \n",
    "Randomly skips entire layers during training.\n",
    "### III.4 Smoothed Dropout: \n",
    "Smooths the dropout mask to reduce noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88a6c82",
   "metadata": {},
   "source": [
    "## IV. Data\n",
    "    - **small batchsize mini-batch SGD**\n",
    "    - **Data Augmentation**: Applies random transformations to the training data to increase diversity.\n",
    "    - **Mixup**: Linearly combines pairs of examples and their labels during training.\n",
    "    - **Cutout**: Randomly masks out rectangular regions of the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb1cb1e",
   "metadata": {},
   "source": [
    "## V. Ensemble Methods\n",
    "Combines predictions from multiple models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be2de7f",
   "metadata": {},
   "source": [
    "## VI. **adversarial method**\n",
    "###  VI.1 Adversarial Training\n",
    "Introduces adversarial examples to improve robustness.\n",
    "###  VI.2 Virtual Adversarial Training**\n",
    "Introduces small perturbations to input samples.\n",
    "\n",
    "**AT和VAT的区别：**\n",
    "            - VAT focuses on generating adversarial examples to increase the model's uncertainty, without explicit adversarial label information.\n",
    "            - Adversarial Training explicitly uses adversarial examples during training to improve robustness against adversarial attacks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb9dfb2",
   "metadata": {},
   "source": [
    "## VII. other methods\n",
    "    1. **Early Stopping**: Monitors a validation metric (e.g., validation loss) and stops training once it starts to degrade.\n",
    "    2. **Label Smoothing**: Modifies target labels during training to be less confident.\n",
    "    3. **Sparse Autoencoders**: Train a NN to learn a sparse representation of the data.\n",
    "    4. **Knowledge Distillation**: Transfers knowledge from a larger, more complex model to a smaller, simpler model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf7647c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:231n] *",
   "language": "python",
   "name": "conda-env-231n-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
