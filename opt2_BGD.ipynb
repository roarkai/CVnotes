{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d33e96ee",
   "metadata": {},
   "source": [
    "# 非凸优化：BGD的opt method\n",
    "\n",
    "凸函数可以收敛到唯一最优解，非凸函数则只能收敛到local min，所以非凸函数的收敛目标是：收敛快，收敛到一个相对好的local min。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01507e65",
   "metadata": {},
   "source": [
    "<font color=norange>**model框架：**\n",
    "$$\n",
    "input \\rightarrow \\underset{optimization}{{\\underbrace{\\overset{f(x,w)}{\\overbrace{feature\\ representation + classifier}} \\rightarrow \\overset{Div,reg}{ \\overbrace{ cost function}}}}} \n",
    "$$</font>\n",
    "\n",
    "**整个model可以拆分为两大类问题：模型结构设计和优化求解**\n",
    "\n",
    "1. 设计一套好的模型结构，包括选择三件套$f(x,w), Div和Reg$，使min expected Loss能尽可能接近min Error这一真实目标\n",
    "2. 选择好的优化方法(optimization method)，能以高效的方式找到相对好的local min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011182a3",
   "metadata": {},
   "source": [
    "## I. 如何设定好的模型结构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61f8367",
   "metadata": {},
   "source": [
    "### I.1 目标转变过程和对应的**Gap of Errors拆解**\n",
    "**target 1**: **expected error/test error**<font color=red>[真实目标]</font>:   $min\\ E(error(f(x, w), y))$ \\\n",
    "<font color=green>$\\Downarrow$ gap1: 让优化问题可以用GD求解</font> \\\n",
    "**target 2**: **expected divergence**<font color=red>[可微]</font>:   $min\\ E(Div(f(x, w), y))$ \\\n",
    "<font color=green>$\\Downarrow$ gap2: Loss(w)是E(Div)的无偏估计，问题不大</font> \\\n",
    "**target 3**: **test error**<font color=red>[可统计/抽样]</font>： $min\\ Loss(w)\\ =\\ min\\ \\frac{1}{N}\\sum_{i=1}^{N}Div(f(x,w),\\ y)$ \\\n",
    "<font color=green>$\\Downarrow$ gap3: **reg**增加了bias以减少Variance</font> \\\n",
    "**target 4**: **training error**：$min\\ Loss(w)\\ =\\ min\\ \\frac{1}{N}\\sum_{i=1}^{N}Div(f(x,w),\\ y)\\ +\\ reg$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c0f9b0",
   "metadata": {},
   "source": [
    "<font color=blue>注：上述变换过程不唯一。</font>如:perceptron将test error转为$min\\ \\frac{1}{N}\\sum_{i=1}^{N}Error(f(x,w),\\ y)$，但这种形式不可微，适用范围有限。\n",
    "\n",
    "<font color=blue>注：**training目标中加上reg item的作用：**</font>用regularization控制overfit。 \\\n",
    "实现方式：reg可以降低training error中第一个part的variance，此时，training error与test error结果会更一致<font color=red>[同小同大，而不是值相等]</font>。所以，在training set上优化后，真正目的是降低的test error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e964b9e",
   "metadata": {},
   "source": [
    "### I.2 **Gap影响因素分析**\n",
    "\n",
    "| error类型 | 公式 | error大小 | 影响因素 |\n",
    "| --- | --- | --- | --- |\n",
    "| truth | $E(error(f^*, y))$ , 其中$f^*=Ey$| 0 | - |\n",
    "| exp error | E(error(f(x, w), y)) | A = Gap1 | f(·)表达力，w优化结果 |\n",
    "| exp div | E(Div(f(x, w), y)) | B = A + Gap2 | div(·), y的表达方式 |\n",
    "| test error | 1/N$\\sum_{i=1}^{N}$Div(f(x,w),y) | C = B + Gap3 | 样本数量和多样性 |\n",
    "| training error | 1/N$\\sum_{i=1}^{N}$Div(f(x,w),y) + reg | D = C + Gap4 | 分布符合reg对应假设 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0c5a75",
   "metadata": {},
   "source": [
    "#### I.2.1 <font color=blue>Gap1：取决于$f(·)$的表达力和w最终优化的结果</font>\n",
    "model的representation power越大，并且w优化结果越好，则A越小。<font color=brown>[这里w的优化结果在下一部分opt method讨论]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f99e07",
   "metadata": {},
   "source": [
    "#### I.2.2 <font color=blue>Gap2：loss function的形式和y的形式决定</font>\n",
    "1. 典型设定 \\\n",
    "(1) MSE直接选择了mean square \\\n",
    "(2) 常见的还有Likelihood, KL divergence, cross entropy \\\n",
    "(3) y的表达方式，比如分类问题中用{0, 1}还是{-1, 1}\n",
    "2. min loss function与min expected error目标不一致的例子 \\\n",
    "(1) 模型设定：二分类问题，输入是一维数据x，用sigmoid function + cross entropy loss \\\n",
    "(2) 不一致问题：\n",
    "<img src=\"pics/sigmoidfunction.png\" alt=\"alt text\" width=\"360\">\\\n",
    "如图，sigmoid function的弯曲形态是固定的$f(x)=\\frac{1}{1+e^{-x}}$不会改变，子啊不同的样本条件下，该曲线会左右移动来实现min loss以完成分类任务。\\\n",
    "假设A类样本都在0点右边，B类样本在0点左边，本来按照图中的原本形态是可以正确分类的。\\\n",
    "但是如果大量的A类型样本取值聚集在非常靠近0点的右侧附近，而B类样本只有少数处于0点左侧附近，那么min loss会将curve往左挤压，导致那些从左侧靠近0点的B类样本被分错。\n",
    "3. 实践影响: <font color=green>divergence带来的问题在classifier中通常不是非常显著</font> \\\n",
    "**[补充说明]**<font color=blue>上例中，如果输入是Wx，也就是多变量，那么问题不大，因为可以通过w的优化找到合适的w，使得linear classifier输出的score不要挤压在0值附近。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e6b482",
   "metadata": {},
   "source": [
    "#### I.2.3 <font color=blue>Gap3：影响小</font>\n",
    "由于experienced risk是expected divergence的无偏估计，对样本量的要求通常能够满足，所以Gap3问题最小"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54d77d6",
   "metadata": {},
   "source": [
    "#### I.2.4 <font color=blue>Gap4：增加的reg项是以bias为代价降低varince</font>\n",
    "<font color=green>Gap4在training data上就是reg item的值。reg的合理性需要weights的真实分布符合reg形态对应的假设。</font>\n",
    "*如果分布符合reg对应的假设，比如使用L2 reg时，分布符合smooth假设*，那么min training error得到的w会让$f(x,w)$的variance更小，代价是$f(x, w)$的bias会增加。*注意variance和bias所指向的评价对象是$f(x,w)$* \n",
    "\n",
    "<font color=norange>**这里的bias和variance影响的是B，以及一定程度上A的大小，而非Gap4：**</font>\n",
    "1. 假如training data的量很大，且其分布代表了真实分布。那么Gap3可以忽略，统计值和期望值可以视为相同值\n",
    "2. 从期望值来看，假如是regression问题，那么Div取MSE合理，还可以忽略Gap2，此时，A = B，用$E(MSE)$表达第一目标expected error合理。\n",
    "bias和variance可以准确表达expected error的大小。\n",
    "3. 由于E(MSE)可以做分解，有trade-off式子：\n",
    "$$E(Div) = E(MSE) = variance + bias + irreducible$$\n",
    "可以通过降低variance和bias来降低E(MSE)，从而降低最终目标expected error。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb392cb",
   "metadata": {},
   "source": [
    "### I.3 Bias-Variance Tradeoff和传统ML的观点\n",
    "#### I.3.1 目标：降低MSE\n",
    "1. <font color=red>**基于MSE的bias-variance分解**</font> \\\n",
    "· 假设：$y = f^{*}(x) + ε$， 其中，ε是与x无关的因素，$E(ε)=0, Var(ε)=σ^{2}$ \\\n",
    "· 样本D的估计量为$\\hat{f}_{D}(x, w)$，简写为$\\hat{f}(x,w)$，或$\\hat{f}$\n",
    "$$\n",
    "\\begin{align}\n",
    "E_{D}(\\hat{f_{D}}(x,w)-y)^{2}\n",
    "&=E_{D}[(\\hat{f_{D}}(x, w)-f^{*}(x))+(f^{*}(x)-y)]^{2}\\\\\n",
    "&=E_{D}(\\hat{f_{D}}(x,w)-f^{*}(x))^{2} + Eε^{2}\\\\    \n",
    "&=E_{D}(\\hat{f_{D}}(x,w)-f^{*}(x))^{2} + σ_{ε}^{2}\\\\\n",
    "&=E_{D}[(\\hat{f_{D}}(x,w)-E_{D}\\hat{f_{D}}(x,w))+(E_{D}\\hat{f_{D}}(x,w)-f^{*}(x))]^{2}+ σ_{ε}^{2}\\\\\n",
    "&=E_{D}[(\\hat{f_{D}}-E_{D}\\hat{f_{D}})+(E_{D}\\hat{f_{D}}-f^{*})]^{2}+ σ_{ε}^{2}\\\\\n",
    "&=E_{D}(\\hat{f_{D}}-E_{D}\\hat{f_{D}})^{2}+E_{D}(E_{D}\\hat{f_{D}}-f^{*})^{2}+ σ_{ε}^{2}\\\\\n",
    "&=sample\\ Var + Bias\\ of\\ estimate + irreducible\\ part\n",
    "\\end{align}\n",
    "$$\n",
    "<font color=red>[注：实验时要用test set来估这里的期望，D=test set。不能用training set]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c183c2",
   "metadata": {},
   "source": [
    "2. 理解variance和bias \\\n",
    "·<font color=blue> **variance:** </font>\\\n",
    "(1) 衡量当训练样本改变时，估计量会发生变化的程度。variance大意味着，样本发生一点点改变，估计结果就会发生很大的改变。\\\n",
    "(2) 通常flexibility越大的model有更大的variance。因为flexibility大的model，会有更多的拟合能力，在min loss的过程中，会拟合noise，而使得variance增大。\\\n",
    "·<font color=blue> **bias:** </font>\\\n",
    "(1) bias衡量的是模型本身与真实函数之间的差异。\\\n",
    "(2) 通常情况下，flexibility越大的模型，bias越小。因为flexibility越大，意味着模型的拟合能力越强，拟合真实函数的可能性就越大。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa26f666",
   "metadata": {},
   "source": [
    "#### I.3.2 传统ML在实验中观察到U形曲线，形成了bias variance trade-off的观点\n",
    "1. 传统ML从实验中观察到：f(x,w)的flexibility与MSE的关系呈U形。在flexibility达到某一最优值后，如果继续增加自由度，反而会让MSE上升。\n",
    "<img src=\"pics/biasvartradeoff.png\" alt=\"alt text\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24666cca",
   "metadata": {},
   "source": [
    "2. 传统ML对U形曲线现象的解释 \\\n",
    "<img src=\"pics/biasvartradeoff2.png\" alt=\"alt text\" width=\"500\"> \\\n",
    "随着模型flexibility提升，bias下降variance上升。这两者下降和上升的相对关系决定了MSE的总体变化是增加还是减少。\\\n",
    "<font color=orange>按照传统ML在简单模型上观察到的经验，flexibility增加的时候，bias一开始减少的快，后面减少得越来越慢。而variance一开始增加得慢，随着flexibility增加会越来越大，这两总情况加总就会得到U形曲线。</font> \\\n",
    "<font color=red>**但是，这个现象在DL中却没有明显证据。**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22cdf43",
   "metadata": {},
   "source": [
    "### I.4 DL结构下最小化MSE的思路\n",
    "#### I.4.1 DL之后的观点改变\n",
    "<font color=blue>**增加flexibility可以减少bias，更大更深的网络可以实现极大的表达力。用大量的数据做拟合，再配合regularization方法减少variance，可以实现MSE的持续下降。**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7c2eec",
   "metadata": {},
   "source": [
    "#### I.4.1 Bias的处理方法：model selection/representation power，data augmentation\n",
    "1. 提高model的表达力，用universal approximator\n",
    "2. 增加数据量：data augmentation，data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a69ec3",
   "metadata": {},
   "source": [
    "#### I.4.2 Variance的处理方法：data, model selection, regularization\n",
    "<font color=norange>第1类variance来源：**由数据样本的随机性带来的**</font>\n",
    "1. 整体数据集本身：**more data，training data和test data分布一致**\n",
    "2. 数据集的使用方式，SGD导致fluctuation：\\\n",
    "(1) 要**shuffle** \\\n",
    "(2) SGD由于sample小，即使shuffle也会有很大的fluctuate，用loss curve呈现的波动会更大。用SGD时要额外处理variance的问题。<font color=green>[详见SGD的opt章节笔记]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9ae7a0",
   "metadata": {},
   "source": [
    "<font color=norange>第2类variance来源：**由model的flexibility带来的，会导致overfit**</font>\n",
    "1. 选择Variance小的model结构和优化方法：比如Cross Entropy+GD比Perceptron算法的Variance更小，但bias更大。\\\n",
    "*说明：* \\\n",
    "Perceptron中增加一个outlier样本会给算法结果带来很大的改变，因此variance很大。但是它的bias很小。相反，用CE或者MLE+Gradient Descent的优化过程受单个样本的影响小，variance小，但是bias会大。<font color=red>**这里也可以看出，loss function和优化方法通常是整体考虑的。**</font>\n",
    "2. 用**regularization**来限制weight的波动：[详见Regularization章节] \\\n",
    "(1) 比如：***weight decay(L2 Norm), weight clipping, dropout(主要用在FC层，CNN中FC被pooling替代后就不适用)*** \\\n",
    "(2) 在ML中，L2 norm与Bayesian等价，可以理解成Bayesian中的先验信息在这里转换成了L2 norm形式的Regularization约束。DL没有Bayesian形式，但仍然可以将L2 norm视为“先验约束”。\\\n",
    "(3) 在DL中，L2对应的“先验约束”意味着，接受“真实分布往往是smooth的”假设"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca506e9e",
   "metadata": {},
   "source": [
    "## II. 用GD做优化面临的问题和solution\n",
    "### II.1 第一类典型问题：loss surface形态不好\n",
    "\n",
    "#### II.1.1 凸函数condition number大可能引起oscillate和收敛慢\n",
    "<font color=green>**solution：Adam = Momentum + RMSprop, input normalization**</font>\n",
    "1. **oscillate** \\\n",
    "(1) 发生原因：单一learning rate时，为了让收敛速度不至于太慢，有的维度$η_{i, opt}<η_{i}<2*η_{i, opt}$，此时，这些维度上梯度符号会正负交互，converge trajectory会发生oscillate。\\\n",
    "<font color=orange>注：和fluctuate不同，oscillate的方向是对的，fluctuate方向是错的。fluctuate是SGD因为抽样的随机性使minibatch得到的梯度$\\ne$偏离真实梯度方向而发生。</font> \\\n",
    "(2) solution: 用梯度的历史平均来决定方向，平滑掉波动，如：**Momentum，Nesterov** \n",
    "2. 收敛慢：\\\n",
    "(1) 发生原因：当$\\eta$时scalar时，为迁就$\\lambda$较大的方向而调低了$\\eta$大小，使得$\\lambda$较小的那些方向上的收敛速度变慢。\\\n",
    "(2) solution：不同的维度用不同的learning rate，adaptive learning rate。如：**RMSprop**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f5694a",
   "metadata": {},
   "source": [
    "#### II.1.2 非凸函数: random restart, SGD + lr decay, skip connection\n",
    "1. saddle point：\\\n",
    "(1) 描述：在该点处gradient=0，迭代停止，但hessian的特征值不是都大于0，而是有正有负。\\\n",
    "(2) solution: Momentum正好解决了这个问题\n",
    "2. inflection point:\\\n",
    "(1) 描述：在该点处gradient=0，，迭代停止，但hessian的特征值也是0。在图形上是连续的“一字型平地”。\\\n",
    "(2) solution: Momentum一定程度上解决了这个问题\n",
    "3. valley：\\\n",
    "(1) 描述：水流冲开山谷形成的深V形的峡谷。这种形态容易发生oscillate。\\\n",
    "(2) solution: adaptive learning rate\n",
    "4. plateau: \\\n",
    "(1) 描述：A plateau is a flat-topped elevated landform从周围平缓地势上拔地而起. \\\n",
    "(2) solution: RMSprop可以减轻这个问题\n",
    "5. poor local min \\\n",
    "(1) 描述：指该local min与global min差距很大 \\\n",
    "(2) solution：**random restart，SGD+learning rate decay**的效果更好，因为SGD寻址的范围更广\n",
    " - **learning rate decay**: 需要的假设是，开口大的位置local min更好，开口小的位置local min不好，所以一开始用大的learning rate可以escape from这些形态不好的位置，直到找到还不错的位置（error rate有了大幅下滑后），再逐步降低learning rate，在选中区域的local min位置converge\n",
    "6. nice local min(指该local min与global min接近)附近的loss surface形态不好 \\\n",
    "(1) 描述：比如high gradient near the min，low gradient far from the min. \\\n",
    "(2) solution：选择好的divergence function和model 结构。如：skip connection[参考：percy liang对loss land scape的研究]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fb63c4",
   "metadata": {},
   "source": [
    "### II.2 第二类典型问题：‘深度’带来的运算问题\n",
    "#### II.2.1 forward propagation和backward propagation都会带来问题\n",
    "1. **Forward Propagation**\n",
    "$$\n",
    "\\begin{align}&z^{(1)}=f(w^{(0)}·z^{(0)}) \\\\&z^{(2)}=f(w^{(1)}·z^{(1)})=f(w^{(1)}·f(w^{(0)}))\\\\&z^{(3)}=f(w^{(2)}·z^{(2)})=f(w^{(2)}·f(w^{(1)}·f(w^{(0)}·z^{(0)})))\\\\...\\\\&z^{(i+1)}=f(w^{(i)}·z^{(i)})=f(w^{(i)}·f(...w^{(3)}·f(w^{(2)}·f(w^{(1)}·f(w^{(0)}·z^{(0)})))))\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ab5ee9",
   "metadata": {},
   "source": [
    "2. **Backward propagation**\n",
    "\n",
    "$$\n",
    "\\begin{align}& \\frac{\\partial Loss}{\\partial z^{(i)}}=dout\\otimes \\bigtriangledown f^{(n)}\\otimes W^{(n)}\\otimes \\bigtriangledown f^{(n-1)}\\otimes W^{(n-1)} ...\\otimes \\bigtriangledown f^{(i)}\\otimes W^{(i)} \\\\& \\frac{\\partial Loss}{\\partial w^{(i)}}=\\frac{\\partial Loss}{\\partial z^{(i+1)}}\\otimes \\frac{\\partial z^{(i+1)}}{\\partial w^{(i)}}=\\frac{\\partial Loss}{\\partial z^{(i+1)}}\\otimes  f^{(i)}\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cea9a85",
   "metadata": {},
   "source": [
    "#### II.2.2 解决forward propagation的方法\n",
    "1. **covariate shift**：**batch norm**\n",
    "    \n",
    "    *LeCun的”Efficient Backpropagation(1998)”中说明了，如果input is whitened，网络训练的收敛速度会更快。*\n",
    "    \n",
    "    - **假设1**：$w_i$的值给定。那么只要最初的输入$z_0=x$的分布不变，则线性变化$w_0·x$的分布也稳定，作为下一层输入的$z_1=f(w_0·x)$的分布也稳定。同理，后续每一层的分布也稳定。\n",
    "    - **放松为假设2**：$w_i$的值在每次迭代时都发生变化，但是能保证线性变化$w_0·x$的分布稳定。此时，作为下一层输入的$z_1=f(w_0·x)$的分布也稳定。同理，后续每一层的分布也稳定。\n",
    "    - **现实情况**：$w_i$的值在每次迭代时都发生变化，导致$w_0·x$的分布变化，从而下一层输入$z_i=f(w_{i-1}·z_{i-1})$的分布也变化。此时，后续每一层的分布都变化。\n",
    "    \n",
    "    **现实情况带来的问题：**\n",
    "    \n",
    "    简化以单层网络为例来说明，如果input的分布总是变化，也就相当于每次输入对应的loss surface都在变化，$w$很难收敛。这比mini-batch SGD的情况还糟糕，因为mini-batch中的分布是一样的，只是因为抽样导致了样本特征变化。batchnorm给出的处理方式是锁定input的分布。相当于一种Regularization，各层w的取值保持各自分布特征(均值和方差)不变。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da6e16e",
   "metadata": {},
   "source": [
    "2. **saturated neurons**：**batchnorm > Xaiver+non-saturated activation+small lr**\n",
    "    \n",
    "    由于activation function本身运算特征和weight初始化取值带来的问题\n",
    "    \n",
    "    - **以tanh、sigmoid为例：**\n",
    "        - **如果$w^{(i)}$中元素scale(eg:体现在variance)小**，eg: $|w^{(i)}|\\approx β < 0.001$\n",
    "            \n",
    "            层度加深后，从某一层开始，后面各层$f(z)$→0\n",
    "            \n",
    "        - **如果$w^{(i)}$中元素scale大**，eg: $|w^{(i)}|\\approx β > 0.1$\n",
    "        \n",
    "        层度加深后，从某一层开始，后面各层$f(z)$→1\n",
    "        \n",
    "        - 上面两种情况下，前向信息传递都失效，$Var(f(z))\\approx0$。还造成梯度消失：\n",
    "        - **sigmoid发生梯度消失：**\n",
    "            - $f(z^{(i)})=0$或=1时，$\\bigtriangledown f_{z^{(i)}}=f(z^{(i)})*(1-f(z^{(i)}))=0$。只要最后一层为0，整个网络的$w$的梯度都为0\n",
    "                - 这里是按元素归零：只要$z_i$中有元素为0，那么$\\bigtriangledown f_{z_i}$在对应位置取0。\n",
    "                - *公式(6)可知，只要某层梯度**整体→0**，比它浅层的所有梯度都→0。*\n",
    "            - 此外，$f(z^{(i)})=0$时，$z^{(i+1)}=w·f(z^{(i)})=0$，公式(7)可知$w^{(i)}$梯度为0。此时，$i$层之后的所有层按照运算特征也都会有$f(z^{(j)})=0$。所以，$i$层之后的各层$w^{(j)}$梯度都为0.\n",
    "                - 但这里要求$f(z_i)$中元素***整体→0***。\n",
    "        - **tanh发生梯度消失：**\n",
    "            - $|f(z^{(i)})|=1$时，$\\bigtriangledown f_{z^{(i)}}=1-f^{2}(z^{(i)})=0$。只要最后一层为0，整个网络的$w$的梯度都为0\n",
    "                - *公式(6)可知，只要某层梯度**整体→0**，比它浅层的所有梯度都→0。*\n",
    "            - $f(z^{(i)})=0$时，$z^{(i+1)}=w·f(z^{(i)})=0$，公式(7)可知$w^{(i)}$梯度为0。此时，$i$层之后的所有层按照运算特征也都会有$f(z^{(j)})=0$。所以，$i$层之后的各层$w^{(j)}$梯度都为0。**和sigmod中不同，此时比$i$层更浅的各层此时还能继续迭代。**\n",
    "                - 但这里也要求$f(z^{(i)})$中元素***整体→0***。\n",
    "    - **理解该问题的3个初步视角**：\n",
    "        1. 选择性质更合适的activation：tanh和sigmoid都有这个问题，tanh没有sigmoid严重，所以RNN用tanh，不用sigmoid。CNN用Relu\n",
    "        2. 让f(z)不趋于相同值，即$Var(f(z))≠0$：Xaiver，MSRA(Kaiming)\n",
    "            - 通过weight initialization可以在刚开始迭代的时候近似满足$Var(z_{i+1}))=Var(z_{i})$，缓解上述问题。ResNet中是$Var(f(w_{i}x_{i})+x)=Var(x_{i})$\n",
    "            - 但一旦开始迭代后，$w$的scale取决于learning rate和梯度的大小，仅靠初始值的上述安排无法保证迭代过程中$Var(z_{i+1}))=Var(z_{i})$继续成立。\n",
    "        3. 在初始化的基础上，为了控制迭代过程中$w$的scale在合理区间，不变得太大，发生saturate，只能用small learning rates，代价是降低了converge rate。\n",
    "        - 但总得来说，**以上方法都没有从本质上解决问题。**\n",
    "    - 一种更强的约束是Batchnorm：\n",
    "        - 让每一层的input的分布保持稳定$distribution(z^{[t+1]}_i)=distribution(z^{[t]}_i)$，而不仅是让每一层input的方差保持层间的一致性$Var(z_{i+1}))=Var(z_{i})$。前者和initialization方法一样可以让$Var(z)≠0$，在此基础上，附加了更多约束。并且还有一个优点是，initialization的各种方法难以在迭代过程中持续保证$Var(z_{i+1}))=Var(z_{i})$。而Batchnorm则可以做到。\n",
    "        - 图示：MLP中不同initial weight scale下，batchnorm效果\n",
    "<img src=\"pics/batchnormvsscale.png\" alt=\"alt text\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35508f13",
   "metadata": {},
   "source": [
    "3. **activation outputs均值不为0：batch norm**\n",
    "    - **SGD时，**导致$w$中同一行元素迭代时，由梯度决定的迭代方向限制在全+和全-空间，发生zigzag，降低迭代速度\n",
    "    - tanh没有这个问题，mini-batch和Batch GD这个问题不大"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cac7006",
   "metadata": {},
   "source": [
    "#### II.2.3 解决backward propagation的方法\n",
    "**梯度消失和爆炸：ResNet， LSTM， Attention** \n",
    "1. **梯度消失**：ReLU, sigmoid, tanh都有$0< \\bigtriangledown f_{z}<1$的特点，随着网络深度增加，$\\frac{\\partial Loss}{\\partial x_{i}}$中元素越来越多趋于0。\n",
    "2. **梯度爆炸**：在RNN中，$W_{i}=W_{j}$，在singular value小于1的方向上shrink vector，大于1的方向上expand vector，后者可能导致gradient explode。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:231n] *",
   "language": "python",
   "name": "conda-env-231n-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
