{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2718b52d",
   "metadata": {},
   "source": [
    "# Image Classification：KNN和softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdf6ce1",
   "metadata": {},
   "source": [
    "# I. KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0630c9f",
   "metadata": {},
   "source": [
    "## I.1 思路\n",
    "取距离（distance）最近的训练集样本类型作为被分类目标的类型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b61c9b",
   "metadata": {},
   "source": [
    "## I.2 算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddc48ce",
   "metadata": {},
   "source": [
    "## I.3 优缺点\n",
    "**优点**：\n",
    "1. 实现简单\n",
    "2. KNN也是universal approximation。因为在有巨大样本数据之后，这些样本构成的图形（比如曲线）就是函数本身的样子。当然，条件是数据量足够。但现实做做不到，因为有“维度的诅咒”。如果因变量有D维，每个维度上有K种取值，就有$K^D$种取值可能。拿到这么多数据不可能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a00986b",
   "metadata": {},
   "source": [
    "**缺点**：\n",
    "1. 时间复杂度高，因为每次做推理都要计算distance。也因此，计算都集中在inference环节，和实践中的需求相反。\n",
    "2. 空间复杂度高，training环节要记住所有training set。\n",
    "3. 当k=1时，很大的问题是不robust。因为只要改变training set中的样本，分类结果就可能发生很大差异。\n",
    "4. **\"distance\"带来的问题：**\\\n",
    "(1) 如果选择L1、L2 norm作为distance的度量工具，在图片分类任务中，这个意义上定义的distance，和人类直觉得到的similarity差异很大。两个例子：\\\n",
    "i. 一张人像用3种不同方式（左移、调黑、挖空部分画面）做处理得到的三张图与原图的distance相同的条件下，看到实际图像的效果差异很大。这体现了用L1norm计算的distance跟人工评价的distance之间的巨大差异。 \n",
    "<img src=\"pics/KNNface.png\" alt=\"alt text\" width=\"560\"/>\n",
    "ii. 用t-SNE(t-SNE用L2 norm)算法把图片投影到二维平面上看，得到的结果显示“距离相近”的图片实际上是那些背景更相似的图片。\n",
    "<img src=\"pics/KNNtSNE.png\" alt=\"alt text\" width=\"560\"/>\n",
    "(2) <font color=red>高维条件下会有“维度的诅咒”问题，因为高维的时候，向量之间的距离如果用L2 norm，则距离倾向于相同，这就让距离评估变得没有意义。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f063591b",
   "metadata": {},
   "source": [
    "## I.4 应用要点\n",
    "### I.4.1 训练注意事项\n",
    "1. 数据预处理：训练集数据要normalize\n",
    "2. 如果数据维度太高，可以先做降维，比如PCA、NCA\n",
    "3. 如果维度高，KNN的推理速度太慢，考虑用approximate Nearest Neighbor library来加速每张图片的nearest neighbor的提取。\n",
    "4. 实际使用KNN的时候，超参数不只是不同的k取值，还应该尝试不同的distance定义，通常会同时试L1和L2norm。\n",
    "5. 整个training data要分成training set和validation set。\\\n",
    "(1) 他们之间的分配要随机。\\\n",
    "(2) 一般70%-90%的样本作为traning set，10%-30%作为validation set.如果超参数数量较大，应该将多一点的样本分给validation set.这样才能尽可能好的比较超参数之间的差异。\\\n",
    "(3) 只要计算资源允许，尽可能做cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1971c9b1",
   "metadata": {},
   "source": [
    "### I.4.2 应用结果\n",
    "| distance | k | Accuracy |others|\n",
    "|----------|----------|----------|----------|\n",
    "| L1 norm | 1 | 38.6% | |\n",
    "| L2 norm | 1 | 35.4% | L2会让单个维度dist差异分布更均衡的2张图片距离更小|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cdec0e",
   "metadata": {},
   "source": [
    "# II. Linear classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23649ce3",
   "metadata": {},
   "source": [
    "**整体算法结构：score function + loss function**\n",
    "1. score function: 也就是linear classifier，作用是map raw data到class score\n",
    "2. loss function：把class score跟label统一起来 \\\n",
    "在此基础上就能将整个问题视为优化问题来求解：min loss with respect to the parameters of the score function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25914df",
   "metadata": {},
   "source": [
    "## II.1. score function\n",
    "### II.1.1 定义形式\n",
    "$$\\begin{align}\n",
    "f(x_i,W, b) & = Wx_i+b \\\\\n",
    "通过bias\\ trick，变成：\\\\\n",
    "f(x_i,W) & = Wx_i\n",
    "\\end{align}$$\n",
    "<font color=blue>**通过W的大小，model可以构成对每个位置上取值的偏好。**</font><font color=orange>比如：对于船这一类型而言，可以想象，在RGB的B所对应的blue分值中，权重w大部分应该是正值，因为船经常出现在蓝色的海洋背景中。相反，R对应的red分值中，很多权重可能就是负值。通过这种W的正负和大小不同，各个class就有了自己类型的“偏好”。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9bfca3",
   "metadata": {},
   "source": [
    "### II.1.2 两种角度理解线性score function(linear classifier)的实践含义\n",
    "#### II.1.2.1 线性代数(几何)的角度：<font color=norange>linear classifier</font>\n",
    "1. **理解线性分类器在做什么**：将每张图片的的raw data(如RGB)展开成一个column vector，可以将其视为高维空间中的一个点。每个class的weights(W中的一个row)对应的score function是高维空间中的一个超平面。\n",
    "2. **局限性**：如果这些class是线性可分的，那么分类的效果就会很好。但实践中通常是线性不可分的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20dee13",
   "metadata": {},
   "source": [
    "#### II.1.2.2 视觉角度：<font color=norange>template/prototype matching</font>\n",
    "1. **理解线性分类器在做什么**：每个class对的W中的一个row，单独把这些row的data取出来画成图片，就构成了各个class的template。在推理阶段做的，就是用图片与每个template比较看跟谁更像。<font color=green>[这里similarity的衡量方式是做inner product]</font>\n",
    "2. **局限性**：如果一个class中的intra-class variation很大，用一个template作为matching的对象，很难完成分类。<font color=brown>比如：马有向左、向右、向前三种典型姿势，他们全部反映在一个template上就会降低template本身作为判断标准的准确性。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176e2c18",
   "metadata": {},
   "source": [
    "## II.2 loss funciton\n",
    "### II.2.1 SVM\n",
    "####  II.2.1.1 思路\n",
    "$$\n",
    "\\begin{align}\n",
    "L_i & =\\sum_j max \\left [0, \\Delta -(s_{y_i}-s_j)\\right ] \\\\\n",
    "min L & = \\frac{1}{N}min \\sum _i L_i \\\\\n",
    "& = \\frac{1}{N}min \\sum _i\\sum_j max \\left [0, \\Delta -(s_{y_i}-s_j)\\right ]\n",
    "\\end{align}\n",
    "$$\n",
    "1. loss function的目标是让正确class的得分比所有其他类型的得分尽可能高一个固定的margin值$\\Delta$。\n",
    "2. $L_i$中取max的作用是让$s_{y_i}$只用比$s_j$高出margin$\\Delta$就行，更高的差异对loss的计算没有影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b10b99",
   "metadata": {},
   "source": [
    "####  II.2.1.2 regularization\n",
    "1. 基础设定中，SVM的loss function有个问题是W取值不唯一。假如W能让Loss(W)=0，那么取任意$W^{'}=\\lambda W, \\lambda>0$都满足$Loss(W^{'})=0$。\n",
    "2. 解决方式是加regularization loss。在这个场景下reg对W的取值引入preference。比如常用的L2 norm可以让W中各个元素的取值尽可能平均，这样可以让所有feature对loss的影响尽可能分散，而不至于出现少数features对应的weights太大，使得这些feature取值的小幅变化导致classification的结果出现较大波动。 \\\n",
    "<font color=blue>**加上reg后的loss function**\n",
    "$$\n",
    "\\begin{align}\n",
    "min L & = \\frac{1}{N}min \\sum _i L_i +\\lambda \\sum \\sum W^2\\\\\n",
    "& = \\frac{1}{N}min \\sum _i\\sum_j max \\left [0, \\Delta -(s_{y_i}-s_j)\\right ]\n",
    "+\\lambda \\sum \\sum W^2 \\\\\n",
    "& = \\frac{1}{N}min \\sum _i\\sum_j max \\left [0, \\Delta -(f(x_i,W)_{y_i}-f(x_i,W)_j)\\right ]\n",
    "+\\lambda \\sum \\sum W^2\n",
    "\\end{align}\n",
    "$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3da110",
   "metadata": {},
   "source": [
    "####  II.2.1.3 应用要点\n",
    "虽然目标函数中有超参数$\\Delta$，但是<font color=blue>**直接设$\\Delta=1$就行**</font>。因为loss function中有两个超参数，他们之间的值在解优化问题时有比例关系。\\\n",
    "<font color=orange>比如，给定$\\Delta=1$得到上式的最优估计。此时，将$\\Delta$调大D倍，再将$\\lambda$调小$D^2$倍，会得到W的最优估计也是此前的D倍。这种比例变化不改变inference阶段的分类结果。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071f7f30",
   "metadata": {},
   "source": [
    "### II.2.2 Softmax\n",
    "<font color=norange>soft differentiable approximation to the hard max function.</font>\n",
    "####  II.2.2.1 思路\n",
    "1. Softmax function\n",
    "$$\n",
    "将score做softmax处理,得到：\\\\\n",
    "\\frac{e^{f_{y_i}}}{\\sum_je^{f_j}} \\\\\n",
    "\\because \\sum_{i=1}^{K}\\frac{e^{f_{i}}}{\\sum_je^{f_j}} = 1, K表示分类的类型数量\\\\\n",
    "\\therefore softmax\\ function可以理解为对P(y|x)的估计: \\\\\n",
    "Q(y_i|x_i;W)=\\frac{e^{f_{y_i}}}{\\sum_je^{f_j}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c86582a",
   "metadata": {},
   "source": [
    "2. cross entropy loss \\\n",
    "(1) 最小化cross entropy就是最小化KL divergence。真实分布P和估计分布Q之间的Cross entropy可以表示为：\n",
    "$$\n",
    "\\begin{align}\n",
    "H(P,Q) \n",
    "& = -\\sum_x P(x)logQ(x) \\\\\n",
    "& = -E_{P(x)}logQ(x)\n",
    "\\end{align}\n",
    "$$\n",
    "(2) 最小化CE与MLE等价：\n",
    "$$\n",
    "\\begin{align}\n",
    "\\underset{w}{argmin}-E_{P(x)}[log{Q_w(x)} ]\n",
    "&=\\underset{Q_w}{argmax}E_{P(x)}[logQ_w(x) ] \\\\\n",
    "&=\\underset{Q_w}{argmax}\\sum_{i=1}^{N}logQ(x_i;w) \n",
    "\\end{align}\n",
    "$$\n",
    "(3)用Softmax function得到的'probability'作为原分布的估计，也就是上式中的Q(x)，则cross entropy可以表示为：\n",
    "$$\n",
    "\\begin{align}\n",
    "L_i \n",
    "& =-log\\frac{e^{f_{y_i}}}{\\sum_je^{f_j}} \\\\\n",
    "& =-f_{y_i} +log\\sum_je^{f_j}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f795bb7e",
   "metadata": {},
   "source": [
    "3. 完整的loss function \\\n",
    "(1) 完整的loss function同样要加上regularization。\n",
    "$$\n",
    "\\begin{align}\n",
    "min\\ L & = \\frac{1}{N}min \\sum _i L_i +\\lambda \\sum \\sum W^2\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "(2) <font color=deeppink>如果用L2 norm，那么此时loss function也可以理解为MAP(maximum a posteriori)。也就是贝叶斯视角下，高斯先验分布条件下的MAP结果。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04150986",
   "metadata": {},
   "source": [
    "####  II.2.2.2 应用要点\n",
    "<font color=blue>计算softmax function经常出现数值不稳定(numeric stability)的问题。\n",
    "1. overflow: </font>因为$e^{f_{j}}$是指数，计算结果可能会非常大。当他们作为分母加入运算时，可能导致float运算溢出。\\\n",
    "**处理方式：**normalization trick。\n",
    "$$score = score - \\underset{i\\in\\{1,...,K\\}}{max}(score_i)$$\n",
    "之后再计算softmax function。\n",
    "这个处理会让score的值小于0，因此不会出现分母过大而溢出的情况。并且这个处理不改变原softmax function的取值大小，因为：\n",
    "$$\\frac{e^{f_{y_i}-C}}{\\sum_je^{f_j-C}} = \\frac{e^{-C}\\times e^{f_{y_i}}}{e^{-C}\\times \\sum_je^{f_j}}= \\frac{e^{f_{y_i}}}{\\sum_je^{f_j}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0ca4e4",
   "metadata": {},
   "source": [
    "2. underflow: $exp(x_{ij}-max(x_i))$可能很小而发生underflow，此时$log(exp(x_{ij}-max(x_i)))$相当于计算$log0$，得到的计算结果是nan。\\\n",
    "**处理方式：**计算probability不用$log\\frac{exp(x_{ij}-max(x_i))}{Σ_jexp(x_{ij}-max(x_i))}$，用$[x_{ij}-max(x_i)]-log(Σ_jexp(x_{ij}-max(x_i)))$，可以避免log0出现"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:231n] *",
   "language": "python",
   "name": "conda-env-231n-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
