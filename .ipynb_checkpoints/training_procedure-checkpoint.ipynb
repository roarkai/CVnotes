{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85aec058",
   "metadata": {},
   "source": [
    "# DNN训练流程\n",
    "\n",
    "1. 先看数据\n",
    "    - 理解数据的分布，观察数据pattern\n",
    "        - 比如：duplicated examples，corrupted images/labels，找到data imbalances/bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaefa84",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9381bbb",
   "metadata": {},
   "source": [
    "2. **gradient check** \n",
    "    1. 用centered形式的公式：$\\frac{df(x)}{dx}=\\frac{f(x+h)-f(x-h)}{2h}$\n",
    "    2. 在比较解析解和数值解的时候，用relative error，不用absolute error\n",
    "        1. 公式是$\\frac{|f'_a-f'_n|}{max(|f'_a|，|f'_n|)}$，分母也可以是add，这两种形式都可以避免$|f'_a|，|f'_n|$有一个为0时，分母为0\n",
    "        2. 但|f'_a|，|f'_n|不能都为0，所以gradient check的时候要跟踪这两个值\n",
    "    3. 用双精度计算数值解\n",
    "    4. 从结果来看，1e-7 通常是比较好的结果。1e-4 > relative error 在有kinks的目标函数中能接受。但是如果目标函数中没有用kink(e.g. use of tanh nonlinearities and softmax), 那么1e-4久太高了。\n",
    "    5. 但网络越深，relative error的合理值就越大。因为error会随着网络加深而累积。如果是10层以上的网络，1e-2 也可能是合理的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2807553",
   "metadata": {},
   "source": [
    "3. **sanity check**：不做BP，只做FP，看网络的initial loss值是否合理\n",
    "    1. 用较小的weights来测试，看不计入reg item的loss是不是和预计的一样。**如：**C类的softmax的initial loss就是logC\n",
    "    2. 加上reg item，增加reg strength应该看到loss增加"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea723f2",
   "metadata": {},
   "source": [
    "4. **在小样本上overfit data：初步确认网络有足够的表达力，且learning process没有问题**\n",
    "    1. 关闭Regularization，在5-10个mini-batch上拿到100%的accuracy。一定要关掉reg，不然loss不可能取到0。\n",
    "    2. 尝试不同的网络结构、learning rate，weight initialization。\n",
    "    3. *注：即使小样本上可以拿到loss为0，也不能保证网络的implementation是正确的。如果在小样本上overfit，但是全样本之后没有泛化，那么就可能implementation是有问题的。 **[???]***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4584615",
   "metadata": {},
   "source": [
    "5. **画图观察learning过程**\n",
    "    1. 画loss curve，看学习过程有没有在持续降低loss\n",
    "        - 横轴要用epoch，不要用iteration。单词iteration的结果受batch的大小影响，epoch没有这个问题\n",
    "    2. 画training acc和validation acc，用来观察overfitting\n",
    "    3. 画每一层activation和gradient的直方图，看有没有saturate或者vanish，以便确认weight initialization是不是有问题\n",
    "    4. 画first-layer visualization，看网络学到了什么东西\n",
    "    5. 看每个weights每一次迭代变化值$dw$的scale相对迭代前$w$的scale的比例。这个值一般在1e-3附件比较合理。如果低于这个值，那么learning rate可能太小了，反之则反。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671a1d25",
   "metadata": {},
   "source": [
    "6. **调参**    \n",
    "    **step1: find LR that makes loss go down，**找到能让loss在100个iteration内快速下降的learning rate用来设定精调范围\n",
    "    \n",
    "    **方法：**用所有的training data，用较小的weight decay，画loss curve，手动选几个不同的learning rate来尝试。\n",
    "    \n",
    "    ***默认尝试值：**1e-1, 1e-2, 1e-3, 1e-4*\n",
    "    \n",
    "    **step2: coarse grid，在step3选中的learning rate附近选2~3个值，以及2~3个weight decay**\n",
    "    \n",
    "    **方法：**用random search训练1~5个epoch，找到结果中比较好的值\n",
    "    \n",
    "    ***默认weight decay：**1e-4, 1e-5, 0。*如果网络复杂度相对数据量较高，weight decay可以大一点，相反可以小一点\n",
    "    \n",
    "    **step3: refine grid**\n",
    "    \n",
    "    **方法：**从上一步中选出最好的参数值，跑10~20个epoch。这一步不用learning rate decay\n",
    "    \n",
    "    **step4: check los curve和acc curve，选择learning rate decay方式**\n",
    "    \n",
    "    1. 根据learn curve的形态来调整learning rate：\n",
    "        1. 在loss还很大的时候就出现plateau，做learning rate decay\n",
    "        2. training accuracy和validation accuracy太接近，underfit：用更大的model，训练更长的时间，也可以考虑增加learning rate。\n",
    "        3. training accuracy和validation accuracy差距太大，overfit：加大weight decay或者其他regulation，增加数据。\n",
    "    2. 如果learning rate出现了突然的divergence，可能是模型有问题，比如梯度消失，training sample中有异常样本，比如损坏的数据等。\n",
    "    \n",
    "    **step5: GOTO step3**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa72789a",
   "metadata": {},
   "source": [
    "7. **Model Ensemble拿到更好的效果：训练多个独立模型，test time的时候用他们预测的均值**\n",
    "    1. 同一个模型，不同的初始化值\n",
    "        - 用Cross-validation来决定最好的超参数，然后多次reinitialization randomly得到多个结果\n",
    "    2. 同一个模型，选择Cross-validation中最好的前几个结果，每个对应一个模型，组成ensemble\n",
    "    3. [低成本]用同一个模型的不同checkpoint来组成ensemble。主要用在训练成本很高的时候\n",
    "    4. [低成本]在训练过程中，记录下exponentailly decaying sum of previous weights during training. 相当于average the state of network over last several iterations.通常这种方式可以获得一点准确率的提升"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:231n] *",
   "language": "python",
   "name": "conda-env-231n-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
