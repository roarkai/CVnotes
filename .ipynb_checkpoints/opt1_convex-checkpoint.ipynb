{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91107c4a",
   "metadata": {},
   "source": [
    "# 凸优化的收敛性质\n",
    "<font color=blue>注1：这里不考虑深度网络，因为深度网络一般都是非凸函数</font> \\\n",
    "<font color=blue>注2：这部分也没有考虑理论目标和训练目标之间的gap，详见非凸收敛分析</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298e467b",
   "metadata": {},
   "source": [
    "## I. L(w) is quadratic, 泰勒精确展开，可以一步求解\n",
    "### I.1 一元二次方程\n",
    "1. **形态**：$f(w)=f(w_{0})+f^{'}(w_{0})(w-w_{0})+\\frac{1}{2}f^{''}(w_{0})(w-w_{0})^{2}$\n",
    "2. **solution**：$w_{min}=w_{0}-\\frac{1}{f^{''}(w_{0})}*f^{'}(w_{0})$\\\n",
    "相当于GD中：$η_{opt}=\\frac{1}{f^{''}(w_{0})}$，取$η=η_{opt}$\\\n",
    "此时有：\n",
    "$$\n",
    "\\begin{align}\n",
    "& η > 2*η_{opt}，diverge\\\\\n",
    "& η < η_{opt}， converge\\\\\n",
    "& η_{opt} < η < 2*η_{opt}，oscillate\n",
    "\\end{align}\n",
    "$$\n",
    "3. **convergence rate**：一步求解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c29c07c",
   "metadata": {},
   "source": [
    "### I.2 多元二次方程：$f(W)=\\frac{1}{2}W^{T}AW+W^{T}B + C$\n",
    "<font color=green>此时$Hessian=\\frac{\\partial^2 f}{\\partial w_i\\partial w_j}=A，condition\\ number=\\frac{max\\lambda_i}{min\\lambda_i}$,$\\lambda$是Hessian Metrix的特征值。</font>\n",
    "#### I.2.1 如果A是diagonal，此时$w_{i}$相互独立\n",
    "1. GD solution: \\\n",
    "$w^{[t+1]} = w^{[t]} -\\eta*\\nabla _{w} f(w)$\n",
    "$\\eta_{i, opt}=\\frac{1}{a_{ii}}$，$a_{ii}$是A的eigen values，也是A对角线上的第i个元素\n",
    "2. $\\eta$的不同形式: \\\n",
    "(1) 如果$\\eta$是scalar，各个维度用同一个learning rate，则：$\\eta<2*\\underset{i}{min}\\ \\eta_{i, opt}$。此时需要迭代，如果condition number太大，收敛会很慢 \\\n",
    "(2) 如果$\\eta$为vector，$\\eta=A^{-1}$, 不同的维度用不同的learning rate。收敛一步到位"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9234d7b",
   "metadata": {},
   "source": [
    "#### I.2.2 如果A不是diagonal：对A做矩阵分解，转diagonal\n",
    "1. 迭代分析: \n",
    "$$\\begin{align} \n",
    "f(W)& = f(W_{min})+\\frac{1}{2}(W-W_{min})^{T}H(W_{min})(W-W_{min})   ……[1]\\\\\n",
    "& =f(W_{min})+\\frac{1}{2}(W-W_{min})^{T}A(W-W_{min}) \\\\\n",
    "迭代式：\\\\\n",
    "w^{[t+1]} & = w^{[t]} -η*\\nabla _{w} f(w) \\\\\n",
    "& =w^{[t]}-η*H(W_{min})(W-W_{min})\\\\\n",
    "& =w^{[t]}-η*A*(W-W_{min})\\\\\n",
    "(w^{[t+1]}-W_{min})&=(I-η*A)(w^{[t]}-W_{min})\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dc2a32",
   "metadata": {},
   "source": [
    "2. 数学处理：change coordinates,取$Λ=ΘAΘ^{T}$，$ν=Θ(W-W_{min})$上面[1]式转化为diagonal form：\n",
    "$$f(ν)\\approx f(0)+\\frac{1}{2}ν^{T}Λv$$\n",
    "对应的迭代方式转换为：\n",
    "$$ν^{[t+1]}=(I-ηΛ)*ν^{[t]}$$\n",
    "solution：取$\\eta$为vector，$\\eta=Λ^{-1}$, 不同的维度用不同的learning rate。代入有：\n",
    "$$ν^{[t+1]}=(I-ηΛ)*ν^{[t]}=0$$\n",
    "因此也可以做到一步收敛到位。\n",
    "3. 问题：<font color=red>但这种方法要对Hessian做分解，复杂度是$O(n^{3})$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612e73b2",
   "metadata": {},
   "source": [
    "## II. L(w) is generic convex function, 泰勒近似展开，Newton’s method\n",
    "### II.1 一维： \n",
    "$$f(w)\\approx f(w_{0})+f^{'}(w_{0})(w-w_{0})+\\frac{1}{2}f^{''}(w_{0})(w-w_{0})^{2}$$\n",
    "#### II.1.1 solution\n",
    "1. Newton’s method \\\n",
    "<font color=red>**Newton’s method跟GD的区别在于，learning rate用$Hessian^{-1}$**</font>\n",
    "$$w^{[t+1]} = w^{[t]} -\\eta*\\nabla_{w} f(w^{[t]})$$\n",
    "(1) 记$\\eta_{opt}=\\frac{1}{f^{''}(w_{0})}$，但此时$\\eta_{opt}$并不是真正的“最优learning rate”，只是近似 \\\n",
    "(2) 取$\\eta=\\eta_{opt}$进行迭代\n",
    "2. GD：略\n",
    "#### II.1.2 convergence rate\n",
    "linearly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5813f23c",
   "metadata": {},
   "source": [
    "### II.2 多维：\n",
    "$$\n",
    "\\begin{align}\n",
    "f(W)\n",
    "& \\approx f(W_{k})+\\nabla_{w}f(W_{k})(W-W_{k})+\\frac{1}{2}(W-W_{k})^{T}H(W_k)(W-W_{k})…[1]\\\\\n",
    "& \\approx f(W_{min})+\\frac{1}{2}(W-W_{min})^{T}H(W_{min})(W-W_{min})…[2]\\\\\n",
    "用[1]式有：\\\\\n",
    "\\nabla_w f(W)\n",
    "& \\approx \\nabla_{w}f(W_{k})+H(W_k)^TW-H(W_k)^TW_k \\\\\n",
    "& \\approx \\nabla_{w}f(W_{k})+H(W_k)W-H(W_k)W_k  , [\\because H^T(W)=H(W)] \\\\\n",
    "& \\approx \\nabla_{w}f(W_{k})+H(W_k)(W-W_k)…[3] \\\\\n",
    "用[2]式有：\\\\\n",
    "\\nabla_w f(W) \n",
    "& \\approx H(W^*)W-H(W^*)W^* \\\\\n",
    "& \\approx H(W^*)(W-W^*)…[4] \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "注：文中$W_{min}就是W^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845f5966",
   "metadata": {},
   "source": [
    "#### II.2.1 solution1： Newton's method\n",
    "$在[3]中代入 \\nabla_w f(W^*) =0，有:$\n",
    "$$W^* \\approx W_k - H(W_k)^{-1}\\nabla_{w}f(W_{k})$$\n",
    "$引出迭代式：w^{[t+1]} = w^{[t]} -\\eta*\\nabla _{w} f(w^{[t]})取$\n",
    "$$\\eta =H(W_k)^{-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cd352e",
   "metadata": {},
   "source": [
    "#### II.2.2 solution2： Batch GD\n",
    "1. 迭代方式：\\\n",
    "将[4]式代入迭代式$w^{[t+1]} = w^{[t]} -\\eta*\\nabla _{w} f(w^{[t]})$有：\n",
    "$$\n",
    "\\begin{align}\n",
    "w^{[t+1]} \n",
    "& = w^{[t]} -\\eta*\\nabla _{w} f(w^{[t]}) \\\\\n",
    "& = w^{[t]} -\\eta*H(W^*)(w^{[t]}-W^*) \\\\\n",
    "w^{[t+1]} - W^*& = (w^{[t]} -W^*) -\\eta*H(W^*)(w^{[t]}-W^*) \\\\\n",
    "\\frac{w^{[t+1]} - W^*}{w^{[t]} - W^*} & =I-\\eta*H(W^*)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25469a4",
   "metadata": {},
   "source": [
    "2. 收敛速率 \\\n",
    "(1) 用数学技巧重构迭代式的等价形式：\\\n",
    "<font color=deeppink>change coordinates，取$Λ=ΘHΘ^{T},ν=Θ(W-W^*)$。*[注，不是whiten]* </font> \\\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\nu =\\Theta (W-W^*)\\Rightarrow W=\\Theta^T\\nu + W^*\\\\\\\n",
    "& 取f(W)=f(\\Theta^T\\nu + W^*)=g(\\nu )\\\\\n",
    "& \\therefore f(W^*)=g(0)\n",
    "\\end{align}\n",
    "$$\n",
    "代入：$$f(W)\\approx f(W_{min})+\\frac{1}{2}(W-W_{min})^{T}H(W_{min})(W-W_{min})$$\n",
    "得：$$\n",
    "g(\\nu )\\approx g(0)+\\frac{1}{2}\\nu ^{T}Λ\\nu$$\n",
    "代入收敛迭代式$$w^{[t+1]} - W^* = (w^{[t]} -W^*) -\\eta*H(W^*)(w^{[t]}-W^*)$$\n",
    "得到对应的用$\\nu$表示的等价迭代式：\n",
    "$$ν^{[t+1]} =(I-\\etaΛ)*ν^{[t]}$$\n",
    "(2) 收敛速率分析：\\\n",
    "t+1步相对t步converge比例为：\n",
    "$$\\frac{ν^{[t+1]}}{ν^{[t]}} =\\frac{W^{[t+1]}-W_{min}}{W^{[t]}-W_{min}}=(I-\\etaΛ)$$\n",
    "在$w_{i}$维度上缩小幅度为：$$1-\\eta*{λ_{i}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecb1a6b",
   "metadata": {},
   "source": [
    "3. $\\eta$的设置\\\n",
    "(1) 如果只能用相同的learning rate：$\\eta$为scalar \\\n",
    " · converge的条件：在任意维度i上满足$|1-η*{λ_{i}}| < 1$ ⇒ $\\eta<\\frac{2}{\\underset{i}{max}{λ_{i}}}$ \\\n",
    " · 在不发生diverge的条件下，实现最快收敛的条件：$\\eta_{opt}=\\frac{1}{\\underset{i}{max}{λ_{i}}}$ \\\n",
    "(2) 如果不同维度用不同的learning rate：取$η$为vector，$\\eta=Λ^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cf8585",
   "metadata": {},
   "source": [
    "4. 收敛分析小结\n",
    "    - condition number：$κ:=\\frac{\\underset{i}{max}{λ_{i}}}{\\underset{i}{min}{λ_{i}}}$\n",
    "    - 如果取$η=η_{opt}=\\frac{1}{\\underset{i}{max}{λ_{i}}}$，$\\frac{ν^{[t+1]}}{ν^{[t]}}=\\frac{W^{[t+1]}-W_{min}}{W^{[t]}-W_{min}}=(I-\\frac{1}{\\underset{i}{max}{λ_{i}}}Λ)$\n",
    "    - 每个维度j上的收敛情况：$R_{j}=\\frac{w_{j}^{[t+1]}-w_{j,min}}{w_{j}^{[t]}-w_{j,min}}=1-\\frac{λ_{j}}{\\underset{i}{max}{λ_{i}}}$\n",
    "    - 收敛最慢的维度：$R_{min}=1-\\frac{\\underset{i}{min}{λ_{i}}}{\\underset{i}{max}{λ_{i}}}=1-\\frac{1}{κ}，(0<R_{min}<1)$\n",
    "        - condition number越大时，$R_{min}$越大，收敛越慢\n",
    "        - 对各个维度j都有：$\\frac{w_{j}^{[t]}-w_{j,min}}{w_{j}^{[0]}-w_{j,min}}=R_{j}^{[t]}*R_{j}^{[t-1]}*···*R_{j}^{[1]}<R_{min}^{t}$\n",
    "        - <font color=blue>t次迭代后，$\\frac{W^{[t]}-W_{min}}{W^{[0]}-W_{min}}<R_{min}^{t}$，因此，凸函数以指数速度收敛</font>\n",
    "        - 对$\\forallε>0$，要让$\\frac{W^{[t]}-W_{min}}{W^{[0]}-W_{min}}<ε$，只要$R_{min}^{t}<ε$，此时$t\\approx\\frac{logε}{logR_{min}}$\n",
    "        - <font color=blue>因此，converge to ε 所需的迭代步数t的复杂度是$O(log(\\frac{1}{ε}))$</font>\n",
    "            - 注：复杂度不是$O(log(ε))$，因为$log(ε)$是负数\n",
    "            - $t\\approx\\frac{logε}{logR}=\\frac{log(\\frac{1}{ε})}{log(\\frac{1}{R})}$，将$log(\\frac{1}{R})$视为常数C，得到$t\\approx \\frac{1}{C}*{log\\frac{1}{ε}}$\n",
    "2. **该方法的问题：**\n",
    "    - 求Hessian和Hessian分解的复杂度都是$O(n^{3})$\n",
    "    - condition number大的时候收敛慢\n",
    "    - 无法确保非凸函数的收敛"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed945f9",
   "metadata": {},
   "source": [
    "#### II.2.3 SGD 收敛速度\n",
    "1. SGD收敛的<font color=green>**充分条件**</font>：<font color=orange>[详见RLnotes的RM章节]</font> \\\n",
    "(1) $\\underset{k}{Σ}\\ η_{k}=∞$ ：可以在整个w的取值空间中寻址。 k表示迭代的步数 \\\n",
    "(2) $\\underset{k}{Σ}\\ η_{k}^{2}<∞$：shrink the step\n",
    "2. 满足上述充分条件的fastest converging series：$\\eta_{opt}\\propto \\frac{1}{k}$\n",
    "3. 收敛速度: <font color=norange>**converge to ε 所需的迭代步数t的复杂度是$O(\\frac{1}{ε})$**</font> \\\n",
    "(1) 假如$\\eta=\\eta_{opt}$，则k次iteration之后：$|\\frac{W^{[t]}-W_{min}}{W^{[0]}-W_{min}}|<\\frac{1}{k}$ \\\n",
    "(2) 对$\\forallε>0$，要让$\\frac{W^{[t]}-W_{min}}{W^{[0]}-W_{min}}<ε$，只要$\\frac{1}{k}=ε$，此时$k= \\frac{1}{ε}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:231n] *",
   "language": "python",
   "name": "conda-env-231n-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
